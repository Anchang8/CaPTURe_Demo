<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="CaPTURe: Cartoon Pose Transfer Using Reverse Attention.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CaPTURe: Cartoon Pose Transfer Using Reverse Attention</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://anchang8.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
      <!--
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://anchang8.github.io">
            Vertebral landmark detection
          </a>
        </div>
      </div>
      -->
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">CaPTURe: Cartoon Pose Transfer Using Reverse Attention</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://anchang8.github.io">Chang-Hyeon An</a><sup>1</sup> and</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=uX_-KBcAAAAJ&hl=en">Hyun-Chul Choi</a><sup>1</sup>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>ICVSLab, Yeungnam University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4374279"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Anchang8"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code(TBD.)</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <center>
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/CaPTURE_Demo1.mp4"
                type="video/mp4">
      </video>
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/CaPTURE_Demo2.mp4"
                type="video/mp4">
      </video>
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/CaPTURE_Demo3.mp4"
                type="video/mp4">
      </video>
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/CaPTURE_Demo4.mp4"
                type="video/mp4">
      </video>
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/CaPTURE_Demo5.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">CaPTURe</span> transfers the target pose (2nd Row) to the target identity (1st Row).
      </h2>
    </div>
    </center>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Most of the previous pose transfer methods require additional input data such as joint keypoints of the target pose extracted by a pre-trained network in a human domain.
          </p>
          <p>
            However, in the fields where cartoon characters are used, such as animation or webtoons, the body proportions and structures of the characters differ from those of actual people, so it is improper to extract additional data using the pre-trained network in the human domain. Even if the network is newly trained in the cartoon domain, expensive data labeling is necessary. As a result, most of the previous pose transfer methods are unsuitable to utilize in the cartoon domain.
          </p>
          <p>
            To address these issues, we propose a Cartoon Pose Transfer network (CaPTURe) that can successfully perform pose transfer in the cartoon domain with only target images and no other input data. Our proposed CaPTURe receives identity and pose source images and extracts information from each source image using effectively designed encoders for cartoon pose transfer. Then the network decodes the identity and pose information with an attention module that makes the network generate the desired conditions more accurately. Here, we adopt the attention mechanism to properly generate the desired identity. On the other hand, we utilize reverse attention to generate the desired pose more precisely, which excludes the identity information of the pose feature and allows us to use only the pose information.
          </p>
          <p>
            Consequently, through comparative experiments using a cartoon domain dataset, CaPTURe shows quantitative results higher by 4.7 % in L1 Distance, 0.1 % in SSIM, and 0.8 % in LPIPS, as well as being more qualitatively superior to the previous state-of-the-art methods. Moreover, we demonstrate that CaPTURe is capable of achieving effective pose transfer not just in the cartoon domains, but also in the human domains, as evidenced by our experiments on a human domain dataset.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<center>
<h2 class="title is-3">Methodology</h2>
</center>
  <table align=center width=600px>
    <tr>
      <td align=center width=600px>
        <center>
          <td><img class="round" style="width:600px" src="./static/images/CaPTURe.png"/></td>
        </center>
      </td>
    </tr>
  </table>
  <table align=center width=600px>
    <tr>
      <td align=center width=600px>
        <center>
          <td><img class="round" style="width:600px" src="./static/images/attnDec.png"/></td>
        </center>
      </td>
    </tr>
  </table>
  <table align=center width=600px>
    <tr>
      <td align=center width=600px>
        <center>
          <td><img class="round" style="width:600px" src="./static/images/attnModule.png"/></td>
        </center>
      </td>
    </tr>
  </table>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2>
        
        <!-- Interpolating. -->
        <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>

<!--
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{an2023capture,
  author    = {Chang-Hyeon An and Hyun-Chul Choi.},
  title     = {CaPTURe: Cartoon Pose Transfer Using Reverse Attention},
  journal   = {Neurocomputing},
  year      = {2023},
}</code></pre>
  </div>
</section>
-->

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
